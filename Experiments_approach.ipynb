{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def load_and_merge_json_files(directory, filename_pattern):\n",
    "    # Pfad zum Verzeichnis mit den JSON-Dateien\n",
    "    path_pattern = f\"{directory}/{filename_pattern}*.json\"\n",
    "    data_list = []  # Liste für das Zusammenführen der Daten aus allen Dateien\n",
    "\n",
    "    # Iterieren über alle Dateien, die dem Muster entsprechen\n",
    "    for filename in glob.glob(path_pattern):\n",
    "        with open(filename, 'r') as file:\n",
    "            try:\n",
    "                data = json.load(file)\n",
    "                data_list.extend(data)  # Daten zur Liste hinzufügen\n",
    "            except:\n",
    "                print(\"Error at loading {} file\".format(filename))\n",
    "\n",
    "    return data_list\n",
    "\n",
    "# Pfad zum Verzeichnis und Dateinamenmuster\n",
    "directory = \"app/Data\"\n",
    "filename_pattern = \"received_data\"\n",
    "# Daten aus Dateien laden und zusammenführen\n",
    "merged_data = load_and_merge_json_files(directory, filename_pattern)\n",
    "\n",
    "# DataFrame aus der zusammengestellten Liste erstellen\n",
    "df = pd.DataFrame(merged_data)\n",
    "df.columns\n",
    "\n",
    "import app.read_data as rd # Read Data\n",
    "import numpy as np\n",
    "import ipaddress\n",
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "import nltk\n",
    "\n",
    "\n",
    "# IPs auf Feature löschen\n",
    "def __checkValidIP(ip):\n",
    "    if ip.startswith(\"http://\") or ip.startswith(\"https://\"):\n",
    "        ip = ip.split(\"//\")[1].split(\":\")[0]\n",
    "    else:\n",
    "        ip = ip.split(\":\")[0]\n",
    "    ip = ip.split(\"/\")[0]\n",
    "    if \"localhost\" in ip:\n",
    "        return True\n",
    "    try:\n",
    "        ipaddress.ip_address(ip)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def __remove_ip(x):\n",
    "    x = str(x)\n",
    "    if __checkValidIP(x):\n",
    "        return \"\"\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "def __remove_ip_features(df):\n",
    "    features_list = [\"Host\", \"Referer\", \"X-Forwarded-For\", \"Origin\"]\n",
    "    headers = df[\"Headers\"].apply(pd.Series)\n",
    "    for feature in features_list:\n",
    "        headers[feature] = headers[feature].apply(__remove_ip)\n",
    "    df = df.drop(columns=\"Headers\")\n",
    "    df = pd.concat([df,headers],axis=1)\n",
    "    return df\n",
    "\n",
    "df = __remove_ip_features(df)\n",
    "\n",
    "\n",
    "\n",
    "# NaN Values ersetzten\n",
    "def fill_nan(df):\n",
    "    for column in df.columns:\n",
    "        df[column].fillna(\"\", inplace=True)\n",
    "        df[column].replace(\"NaN\",\"\",inplace=True)\n",
    "        df[column].replace(\"nan\",\"\",inplace=True)\n",
    "    return df\n",
    "\n",
    "df = fill_nan(df)\n",
    "\n",
    "\n",
    "\n",
    "# Original DF speichern\n",
    "df_origin = df\n",
    "\n",
    "\n",
    "\n",
    "# Entfernen von Timestamp, SourceIP und SourcePort\n",
    "def __remove_features(df):\n",
    "    selected_features = [col for col in df.columns if col not in ['Timestamp', 'SourceIP', 'SourcePort']]\n",
    "    return df[selected_features].copy()\n",
    "\n",
    "df = __remove_features(df)\n",
    "\n",
    "\n",
    "\n",
    "# Markieren von Duplikaten und Entfernen davon\n",
    "df[\"Duplicate\"] = df.duplicated(keep=False)\n",
    "df.drop_duplicates(keep='first',inplace=True,ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "# Leere Spalten löschen\n",
    "df = df.dropna(axis=1, how='all')\n",
    "\n",
    "def delete_cluster_columns(df_temp):\n",
    "    columns_to_drop = [col for col in df_temp.columns if col.startswith(\"Ex_\")]\n",
    "    df_temp.drop(columns=columns_to_drop, inplace=True)\n",
    "    return df_temp\n",
    "\n",
    "def concatenate_rows(df):\n",
    "    def concatenate_row(row):\n",
    "        return ''.join(row.astype(str))\n",
    "    arr_string_full_concat = df.apply(concatenate_row, axis=1).values\n",
    "    return arr_string_full_concat\n",
    "\n",
    "dataset_1 = concatenate_rows(df)\n",
    "\n",
    "def concatenate_equalize_length(df):\n",
    "    def concatenate_row(row, max_lengths):\n",
    "        equalized_row = [str(val).ljust(max_lengths.iloc[idx]) for idx, val in enumerate(row)]\n",
    "        return ''.join(equalized_row)\n",
    "\n",
    "    max_lengths = df.apply(lambda x: x.astype(str).str.len()).max()\n",
    "    concatenated_array = df.apply(lambda row: concatenate_row(row, max_lengths), axis=1).values\n",
    "    return concatenated_array\n",
    "\n",
    "dataset_2 = concatenate_equalize_length(df)\n",
    "\n",
    "dataset_3 = df.drop(columns=[\"Duplicate\"])\n",
    "dataset_3 = delete_cluster_columns(dataset_3)\n",
    "\n",
    "dataset_4 = df.drop(columns=[\"Duplicate\", \"User-Agent\"])\n",
    "dataset_4 = delete_cluster_columns(dataset_4)\n",
    "\n",
    "dataset_5 = df.drop(columns=[\"Duplicate\", \"User-Agent\"])\n",
    "dataset_5 = delete_cluster_columns(dataset_5)\n",
    "for column in dataset_5.columns:\n",
    "    counts = dataset_5[column].value_counts()\n",
    "    frequent_entries = counts[counts >= 20].index.tolist()\n",
    "    dataset_5.loc[dataset_5[column].isin(frequent_entries), column] = \"\"\n",
    "    dataset_5 = dataset_5.dropna(axis=1, how='all')\n",
    "\n",
    "dataset_6 = dataset_4\n",
    "dataset_6['Path'] = dataset_6['Path'].str[1:]\n",
    "remove_extensions = [\".php\", \".xml\", \".html\", \"index\", \".js\", \".ico\", \".txt\", \".env\"]\n",
    "for substring in remove_extensions:\n",
    "    dataset_6['Path'] = dataset_6['Path'].str.replace(substring, '', regex=False)\n",
    "\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def bag_of_chars(str):\n",
    "    # Combine all printable ASCII characters and digits\n",
    "    chars = string.ascii_letters + string.digits + string.punctuation\n",
    "\n",
    "    # Create a bag-of-chars for each string\n",
    "    vectorizer = CountVectorizer(analyzer='char', lowercase=False, vocabulary=list(chars))\n",
    "    bag = vectorizer.fit_transform([str]).toarray().flatten()\n",
    "    return bag\n",
    "\n",
    "def normalized_levenshtein_distance(s1, s2):\n",
    "    # Berechnung der Levenshtein-Distanz\n",
    "    m, n = len(s1), len(s2)\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "    \n",
    "    for i in range(m + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(n + 1):\n",
    "        dp[0][j] = j\n",
    "    \n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            cost = 0 if s1[i-1] == s2[j-1] else 1\n",
    "            dp[i][j] = min(dp[i-1][j] + 1,\n",
    "                           dp[i][j-1] + 1,\n",
    "                           dp[i-1][j-1] + cost)\n",
    "    \n",
    "    levenshtein_distance = dp[m][n]\n",
    "    # Maximale mögliche Distanz ist die Länge des längeren Strings\n",
    "    max_distance = max(m, n)\n",
    "    # Normalisierung der Levenshtein-Distanz\n",
    "    normalized_distance = 1 - levenshtein_distance / max_distance\n",
    "    return normalized_distance\n",
    "\n",
    "def array_similarity(array1, array2):\n",
    "    # Anzahl der nicht vorhandenen Zeichen in beiden Arrays zählen\n",
    "    non_matching_chars = sum(1 for a1, a2 in zip(array1, array2) if a1 == 0 and a2 == 0)\n",
    "    # Anzahl der übereinstimmenden Zeichen zählen\n",
    "    matching_chars = sum(min(a1, a2) for a1, a2 in zip(array1, array2))\n",
    "    # Gesamtsumme der Zeichen in beiden Arrays zählen\n",
    "    total_chars = sum(array1) + sum(array2)\n",
    "    # Übereinstimmung in Prozent berechnen\n",
    "    # Bei zwei leeren Strings\n",
    "    if total_chars == 0:\n",
    "        return 1\n",
    "    return matching_chars / (abs(total_chars)/2)\n",
    "\n",
    "def bag_of_chars_and_selfdesigned_similarity(str1, str2):\n",
    "    bag1 = bag_of_chars(str1)\n",
    "    bag2 = bag_of_chars(str2)\n",
    "\n",
    "    # Calculate the cosine similarity\n",
    "    return array_similarity(bag1, bag2)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import OPTICS\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import numpy as np\n",
    "\n",
    "def calculate_silhouette_coefficient(distance_matrix, labels):\n",
    "    return silhouette_score(distance_matrix, labels, metric=\"precomputed\")\n",
    "\n",
    "def optics_clustering(dm_ex, min_samples, xi, eps):\n",
    "    optics = OPTICS(min_samples=min_samples, xi=xi, max_eps=eps)\n",
    "    # Fit the model to the data\n",
    "    optics.fit(dm_ex)\n",
    "    # Return the clustering labels\n",
    "    return optics.labels_\n",
    "\n",
    "def find_optimal_parameters_optics(data, min_samples_range=[2], xi_range=[0.01], eps_range=[2]):\n",
    "    optimal_params = {'min_samples': min_samples_range[0], 'xi': xi_range[0],'eps': eps_range[0]}\n",
    "    max_silhouette = -1\n",
    "    best_labels = None\n",
    "    for min_samples in min_samples_range:\n",
    "        for xi in xi_range:\n",
    "            for eps in eps_range:\n",
    "                print(\"Parameters - Xi: {} Eps: {}\".format(xi,eps))\n",
    "                optics = OPTICS(min_samples=min_samples, xi=xi,max_eps=eps).fit(data)\n",
    "                labels = optics.labels_\n",
    "                n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "                \n",
    "                if 1 < n_clusters < len(data) - 1:\n",
    "                    silhouette = silhouette_score(data, labels)\n",
    "                    if silhouette > max_silhouette:\n",
    "                        max_silhouette = silhouette\n",
    "                        optimal_params['min_samples'] = min_samples\n",
    "                        optimal_params['xi'] = xi\n",
    "                        optimal_params['eps'] = eps\n",
    "                        best_labels = labels\n",
    "    return optimal_params, max_silhouette, best_labels, optics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prefiltering mit Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def add_findings_df_prefiltered(results):\n",
    "    outlier_ind = dict()\n",
    "    for column in results:\n",
    "        matching_rows = results[results[column] == True]\n",
    "        indices = matching_rows.index\n",
    "        outlier_ind[column] = indices\n",
    "    for column in outlier_ind:\n",
    "        for idx in outlier_ind[column]:\n",
    "            if df_prefiltered.at[idx,\"Outlier\"] is None:\n",
    "                df_prefiltered.at[idx,\"Outlier\"] = [column]\n",
    "            elif column not in df_prefiltered.at[idx,\"Outlier\"]:\n",
    "                df_prefiltered.at[idx,\"Outlier\"].append(column)\n",
    "\n",
    "def find_long_values(df, whitelist_Features, threshold):\n",
    "    results = pd.DataFrame(index=df.index)\n",
    "    # Iterate over each column in the DataFrame\n",
    "    for col in df.columns:\n",
    "        if col not in whitelist_Features:\n",
    "            # Calculate the threshold length for this column\n",
    "            col_len = df[col].apply(len)\n",
    "            threshold_len = col_len.quantile(threshold)\n",
    "            exceptionally_long = col_len > threshold_len\n",
    "            results[f'{col}'] = exceptionally_long\n",
    "\n",
    "    add_findings_df_prefiltered(results)\n",
    "    return results\n",
    "\n",
    "import re\n",
    "\n",
    "def find_special_chars(df, whitelist_Features=[], threshold=0.1):\n",
    "    results = pd.DataFrame(index=df.index)\n",
    "    # Iterate over each column in the DataFrame\n",
    "    for col in df.columns:\n",
    "        if col not in whitelist_Features:\n",
    "            # Calculate the threshold length for this column\n",
    "            col_values = df[col]\n",
    "            col_values = col_values[col_values.str.len() > 5]\n",
    "            special_chars = re.compile(r'[^\\w\\s]')\n",
    "            num_special_chars = col_values.apply(lambda x: len(special_chars.findall(x)))\n",
    "            threshold_num_special_chars = num_special_chars.quantile(threshold)\n",
    "            exceptionally_high_special_chars = num_special_chars > threshold_num_special_chars\n",
    "            results[f'{col}'] = exceptionally_high_special_chars\n",
    "    add_findings_df_prefiltered(results)\n",
    "    return results\n",
    "\n",
    "def find_spaces(df, whitelist_Features=[], threshold=0.5):\n",
    "    results = pd.DataFrame(index=df.index)\n",
    "    # Iterate over each column in the DataFrame\n",
    "    for col in df.columns:\n",
    "        if col not in whitelist_Features:\n",
    "            # Calculate the threshold length for this column\n",
    "            col_values = df[col]\n",
    "            col_values = col_values[col_values.str.len() > 5]\n",
    "            num_spaces = col_values.apply(lambda x: x.count(' '))\n",
    "            threshold_num_spaces = num_spaces.quantile(threshold)\n",
    "            exceptionally_high_spaces = num_spaces > threshold_num_spaces\n",
    "            results[f'{col}'] = exceptionally_high_spaces\n",
    "    add_findings_df_prefiltered(results)\n",
    "    return results\n",
    "\n",
    "def find_keywords(df, keywords=['cd', 'curl', 'rm ', 'cmd', 'wget', 'rm%20', 'Base64', 'shell']):\n",
    "    results = pd.DataFrame(index=df.index)\n",
    "    # Iterate over each column in the DataFrame\n",
    "    for col in df.columns:\n",
    "            has_keyword = df[col].apply(lambda x: any(keyword in x for keyword in keywords))\n",
    "            results[f'{col}'] = has_keyword\n",
    "    add_findings_df_prefiltered(results)\n",
    "    return results\n",
    "\n",
    "def remove_duplicates_from_outlier(df_temp):\n",
    "    # Check if 'Duplicate' and 'Outlier' columns exist in the DataFrame\n",
    "    if 'Duplicate' not in df_temp.columns or 'Outlier' not in df_temp.columns:\n",
    "        raise ValueError(\"DataFrame must contain 'Duplicate' and 'Outlier' columns\")\n",
    "\n",
    "    for index, row in df_temp.iterrows():\n",
    "        if row['Duplicate'] == True:\n",
    "            df_temp.at[index, 'Outlier'] = []\n",
    "    return df_temp\n",
    "\n",
    "df_prefiltered = df.copy()\n",
    "df_prefiltered[\"Outlier\"] = None\n",
    "# Prefiltering\n",
    "whitelist_Features = [\"Path\"]\n",
    "results = find_long_values(dataset_6,whitelist_Features, 0.995)\n",
    "results = find_special_chars(dataset_6,threshold=0.95)\n",
    "results = find_spaces(dataset_6,threshold=0.90)\n",
    "results = find_keywords(dataset_6)\n",
    "\n",
    "df_prefiltered = remove_duplicates_from_outlier(df_prefiltered)\n",
    "\n",
    "# Assuming df is your DataFrame and \"Outlier\" is the column of interest\n",
    "exploded_df = df_prefiltered['Outlier'].explode().dropna().reset_index(drop=True)\n",
    "unique_values = exploded_df.unique()\n",
    "\n",
    "dfs = {}\n",
    "for col in unique_values:\n",
    "    df_prefiltered['Outlier'] = df_prefiltered['Outlier'].apply(lambda x: x if x is not None else [])\n",
    "    dfs[col] = df_prefiltered[df_prefiltered['Outlier'].apply(lambda x: col in x)]\n",
    "from sklearn.metrics import silhouette_score\n",
    "def distance_matrix_lev(array):\n",
    "    # Get the length of the array\n",
    "    n = len(array)\n",
    "    # Initialize the distance matrix as an empty matrix\n",
    "    dist_matrix = np.zeros((n, n))\n",
    "    # Get the unique values in the array\n",
    "    unique_values = np.unique(array)\n",
    "    # Compute the cosine similarity between each pair of strings in the unique values\n",
    "    for i, value1 in enumerate(unique_values):\n",
    "        for j, value2 in enumerate(unique_values):\n",
    "            if i <= j:\n",
    "                if value1 != value2:\n",
    "                            # Calculate the cosine similarity\n",
    "                    if len(value1) == 0 ^ len(value2) == 0:\n",
    "                        similarity = 0\n",
    "                    if len(value1) == 0 and len(value2) == 0:\n",
    "                        similarity = 1\n",
    "                    else:\n",
    "                        similarity = normalized_levenshtein_distance(value1,value2)\n",
    "                    # Get the indices of value1 and value2 in the input array\n",
    "                    idx1 = np.where(array == value1)[0]\n",
    "                    idx2 = np.where(array == value2)[0]\n",
    "                    # Assign the calculated similarity to the correct positions in the distance matrix\n",
    "                    for n in idx1:\n",
    "                        for m in idx2:\n",
    "                            dist_matrix[n, m] = 1 - similarity\n",
    "                            dist_matrix[m, n] = 1 - similarity\n",
    "    return dist_matrix\n",
    "\n",
    "\n",
    "def process_column_data_prefiltering(column_name,data):\n",
    "    ex_num = f\"Cluster_Prefiltering_{column_name}\"\n",
    "    if len(data) > 1:\n",
    "        dm_ex = distance_matrix_lev(data)\n",
    "        eps, max_sil, labels = find_optimal_parameters_optics(dm_ex)\n",
    "        print(\"For Category {} Requests: {}  the best eps: {} with sil-score: {}\".format(column_name,len(dfs[column_name]), eps, max_sil))\n",
    "        dfs[column_name].loc[:,ex_num] = labels  # Speichere die Labels im entsprechenden DataFrame\n",
    "    else:\n",
    "        dfs[column_name].loc[:,ex_num] = -1\n",
    "\n",
    "\n",
    "\n",
    "for col in unique_values:\n",
    "    data = dfs[col][col]\n",
    "    process_column_data_prefiltering(col,data)\n",
    "\n",
    "ex_num = \"Cluster_Prefiltering\"\n",
    "\n",
    "for col in unique_values:\n",
    "    new_col_name = \"{}_{}\".format(ex_num, col)\n",
    "    if new_col_name not in df_prefiltered.columns:\n",
    "        df_prefiltered[new_col_name] = np.nan  # Verwenden Sie np.nan für fehlende Daten\n",
    "    df_prefiltered.loc[dfs[col].index, new_col_name] = dfs[col][new_col_name]\n",
    "# Angenommen, fill_nan ist eine Funktion, die NaN-Werte behandelt\n",
    "df_prefiltered = fill_nan(df_prefiltered)\n",
    "\n",
    "def analyse_prefiltered_outlier(df_prefiltered):\n",
    "    # Initialisierung der Spalte 'Outlier_Detected' mit False\n",
    "    df_prefiltered['Outlier_Detected'] = None\n",
    "    # Initialisiere 'Outlier_Category' als None, um später Arrays zu speichern\n",
    "    df_prefiltered['Outlier_Category'] = None\n",
    "\n",
    "    for col in df_prefiltered.columns[df_prefiltered.columns.str.startswith('Cluster_')]:\n",
    "        # Finde die Indizes der Ausreißer (Werte von -1) in der aktuellen Spalte\n",
    "        outlier_indices = df_prefiltered[df_prefiltered[col] == -1].index\n",
    "        non_outlier_indices = df_prefiltered[df_prefiltered[col] != \"\"].index\n",
    "        for idx in non_outlier_indices:\n",
    "            if df_prefiltered.at[idx, 'Outlier_Detected'] != True:\n",
    "                df_prefiltered.at[idx, 'Outlier_Detected'] = False\n",
    "\n",
    "        for idx in outlier_indices:\n",
    "            # Markiere die Zeile als Outlier\n",
    "            df_prefiltered.at[idx, 'Outlier_Detected'] = True\n",
    "\n",
    "            # Wenn 'Outlier_Category' noch nicht initialisiert wurde, füge die erste Spalte hinzu\n",
    "            if df_prefiltered.at[idx, 'Outlier_Category'] is None:\n",
    "                df_prefiltered.at[idx, 'Outlier_Category'] = [col.removeprefix(ex_num+\"_\")]\n",
    "            # Wenn die Spalte noch nicht in 'Outlier_Category' enthalten ist, füge sie hinzu\n",
    "            elif col not in df_prefiltered.at[idx, 'Outlier_Category']:\n",
    "                df_prefiltered.at[idx, 'Outlier_Category'].append(col.removeprefix(ex_num+\"_\"))\n",
    "    return df_prefiltered\n",
    "\n",
    "df_prefiltered = analyse_prefiltered_outlier(df_prefiltered)\n",
    "df_prefiltered = fill_nan(df_prefiltered)\n",
    "df['Outlier_Detected'] = df_prefiltered['Outlier_Detected']\n",
    "df['Outlier_Category'] = df_prefiltered['Outlier_Category']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temp Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = dataset_6\n",
    "for column in df_temp:\n",
    "    unique_values = df_temp[column].value_counts()\n",
    "    file_name = f\"output_{column}.txt\"\n",
    "    with open(file_name, 'w') as file:\n",
    "        for value, count in unique_values.items():\n",
    "            file.write(f\"{value}: {count}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distanzmatritzen und Clustering für jedes Feature durchführen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "def distance_matrix_self(array):\n",
    "    # Get the length of the array\n",
    "    n = len(array)\n",
    "    # Initialize the distance matrix as an empty matrix\n",
    "    dist_matrix = np.zeros((n, n))\n",
    "    # Get the unique values in the array\n",
    "    unique_values = np.unique(array)\n",
    "    # Compute the cosine similarity between each pair of strings in the unique values\n",
    "    for i, value1 in enumerate(unique_values):\n",
    "        for j, value2 in enumerate(unique_values):\n",
    "            if i <= j:\n",
    "                if value1 != value2:\n",
    "                            # Calculate the cosine similarity\n",
    "                    if len(value1) == 0 ^ len(value2) == 0:\n",
    "                        similarity = 0\n",
    "                    if len(value1) == 0 and len(value2) == 0:\n",
    "                        similarity = 1\n",
    "                    else:\n",
    "                        similarity = bag_of_chars_and_selfdesigned_similarity(value1,value2)\n",
    "                    # Get the indices of value1 and value2 in the input array\n",
    "                    idx1 = np.where(array == value1)[0]\n",
    "                    idx2 = np.where(array == value2)[0]\n",
    "                    # Assign the calculated similarity to the correct positions in the distance matrix\n",
    "                    for n in idx1:\n",
    "                        for m in idx2:\n",
    "                            dist_matrix[n, m] = 1 - similarity\n",
    "                            dist_matrix[m, n] = 1 - similarity\n",
    "    return dist_matrix\n",
    "\n",
    "df_features = {}\n",
    "\n",
    "def process_column_data_features(column_name,data):\n",
    "    ex_num = f\"Cluster_Features_{column_name}\"\n",
    "\n",
    "    if len(data) > 1:\n",
    "        print(\"Calc Distancematrix\")\n",
    "        dm_ex = distance_matrix_self(data)\n",
    "        print(\"Clustering...\")\n",
    "        eps, max_sil, labels, optics = find_optimal_parameters_optics(dm_ex)\n",
    "        print(\"For Category {} Requests: {}  the best eps: {} with sil-score: {}\".format(column_name,len(df[column_name]), eps, max_sil))\n",
    "        outlier_indices = np.where(labels == -1)[0]\n",
    "        for idx in outlier_indices:\n",
    "            df.at[idx,'Outlier_Detected'] = True  \n",
    "            if len(df.at[idx, 'Outlier_Category']) == 0:\n",
    "                df.at[idx, 'Outlier_Category'] = [column_name]\n",
    "            elif column_name not in df.at[idx, 'Outlier_Category']:\n",
    "                df.at[idx, 'Outlier_Category'].append(column_name)\n",
    "    else:\n",
    "        print(\"{} skipped due to length 1\".format(column_name))\n",
    "    return optics\n",
    "\n",
    "\n",
    "unique_values = dataset_6.columns\n",
    "\n",
    "for col in unique_values:\n",
    "    df_features[col] = None\n",
    "\n",
    "for col in unique_values:\n",
    "        print(\"Calculate for Feature: {}\".format(col))\n",
    "        data = dataset_6[col]\n",
    "        optics = process_column_data_features(col,data)\n",
    "\n",
    "\n",
    "#1143 Minuten"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
